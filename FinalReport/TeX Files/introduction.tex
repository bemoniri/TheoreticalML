\chapter{مقدمه}
در این پروژه، به بررسی مقاله‌ی 
\cite{NIPS2019_9005}
می‌پردازیم.
مسئله‌ی اصلی مطرح‌شده در این مقاله، مسئله‌ی کاهش بعد است. کاهش بعد به معنای معرّفی نگاشتی است که داده‌ها را از فضای اصلی، به فضایی با بعد کمتر بنگارد، به گونه‌ای که ویژگی‌های اصلی داده‌ها که می‌توانند داده‌ها را از یکدیگر متمایز کنند، در فضای کاهش بعد یافته حضور داشته باشند. هدف اصلی کاهش بعد، صرفه‌جویی در حافظه‌ی موردنیاز برای ذخیره‌سازی داده‌ها و همچنین پردازش آسان‌تر و سریع‌تر داده‌ها است.

یکی از روش‌های متداول و معروف برای کاهش بعد، روش 
\lr{Principal Component Analysis (PCA)}
است. این روش برای اوّلین بار توسّط 
\lr{Pearson}
و در سال ۱۹۰۱ معرّفی شد
\cite{karl1901liii}.
در این روش اگر فرض کنیم داده‌ها به صورت بردارهای 
\lr{$\bx_i \in \R^d$}
داده شده‌اند، هدف آن است که یک ماتریس
\lr{$W \in \R^{n\times d}$}
بیابیم، به صورتی که 
\lr{$\bx'_i=W\bx_i\in \R^n$}
ویژگی‌های اصلی داده‌ها را حفظ کند. مزیّت اصلی روش 
\lr{PCA}
در تفسیرپذیر 
\lr{(interpretable)}
بودن ویژگی‌ها در فضای کاهش بعد یافته است. به تعبیر دیگر با داشتن ماتریس 
\lr{$W$}
می‌توان دریافت که ویژگی‌های داده‌ها در فضای اصلی، به چه صورت باهم ترکیب شده‌اند و فضای کاهش بعد یافته را ساخته‌اند. ضعف اصلی روش
\lr{PCA}
در آنست که این روش، تنها می‌تواند روابط خطّی بین ویژگی‌ها را استخراج کند و از استخراج روابط غیرخطّی بین ویژگی‌ها ناتوان است.

برای استخراج روابط غیرخطّی در ویژگی‌ها، باید از کرنل‌ها استفاده کنیم. به این معنا که ابتدا با کمک یک کرنل مناسب، داده‌ها را به فضایی با بعد بالاتر بنگاریم و سپس در آن فضا، الگوریتم 
\lr{PCA}
را اجرا کنیم تا بتوانیم بعد داده‌ها را کم کرده و روابط غیرخطّی بین ویژگی‌ها را نیز شناسایی کنیم. 
\cite{scholkopf1997kernel}
این الگوریتم را 
\lr{Kernel PCA (KPCA)}
می‌نامند. 
\cite{scholkopf1998nonlinear,barshan2011supervised}
الگوریتم 
\lr{KPCA}
بسیار قدرتمند است، ولی دو ضعف دارد:
\begin{enumerate}
	\item 
	اگر داده‌ها برچسب داشته باشند، این الگوریتم نمی‌تواند از برچسب‌های آن‌ها برای کاهش بعد دقیق‌تر استفاده‌ای کند
	\item 
	چون 
	\lr{PCA}
	در یک فضای با بعد بالاتر از ابعاد داده‌ها اجرا می‌شود، نمی‌توان به دقّت گفت که هرکدام از ویژگی‌ها در فضای کاهش بعد یافته، چگونه به ویژگی‌های اصلی داده‌ها مربوط می‌شوند. 
\end{enumerate}

این دو مشکل توسّط الگوریتم
\lr{Interpretable Kernel Dimension Reduction (IKDR)}
حل می‌شوند. در این الگوریتم، برخلاف
\lr{KPCA}،
داده‌ها ابتدا کاهش بعد داده می‌شوند و بعد از آن  کرنل روی آن‌ها عمل می‌کند. همچنین این الگوریتم از برچسب‌های داده‌ها هم استفاده می‌کند و ماتریس 
\lr{$W$}ای
را پیدا می‌کند که 
\lr{$XW$}
و
\lr{$Y$}
تا حدّ امکان به هم وابسته شوند. برای تعیین میزان وابستگی هم از معیار
\lr{Hilbert Schmidt Independence Criterion (HSIC)}
\cite{gretton2005measuring} 
استفاده می‌کند. 
\cite{wu2018iterative,fukumizu2009kernel,barshan2011supervised, masaeli2010transformation,scholkopf1998nonlinear,niu2011dimensionality,gangeh2016semi,chang2017clustering,niu2010multiple,song2012feature}
در حقیقت هدف الگوریتم 
\lr{IKDR}
حل‌کردن این مسئله است:
\begin{equation}\label{eq1}
\max_X \;\; \HSIC(XW,Y) \qquad \qquad\text{\lr{s.t. }} W^\top W = I
\end{equation}
مشکلی که وجود دارد، آن‌ است که این مسئله از نظر محاسباتی پیچیده است، زیرا محدّب نیست و به شدّت غیرخطّی است. 

با توجّه به شرط
\lr{$W^\top W = I$}،
این مسئله یک مسئله‌ی بهینه‌سازی روی رویه است. در نتیجه قید را می‌توان به صورت یک
\lr{Stiefel Manifold}
یا
\lr{Grassmann manifold}
در نظر گرفت. 
\cite{james1976topology,nishimori2005learning,edelman1998geometry}

راه‌حل‌های مختلفی برای حلّ این مسئله پیشنهاد شده است. در
\cite{boumal2011rtrmc} 
مسئله‌ی مشابهی روی یک
\lr{Grassmann manifold}
در نظر گرفته شده و از روش‌های مرتبه‌ی اوّل و دوم ناحیه اطمینان ریمانی
برای حلّ آن استفاده شده است. در 
\cite{theis2009soft}
از روش ناحیه اطمینان برای کمینه‌کردن یک تابع هزینه روی یک
\lr{Stiefel Manifold}
استفاده شده است. روش به کار رفته در
\cite{wen2013feasible}،
به این صورت است که
\lr{Stiefel Manifold}
باز می‌شود و به صورت یک صفحه‌ی تخت درمی‌آید و مسئله‌ی بهینه‌سازی روی این صفحه حل می‌شود. روش‌های مبتنی بر 
\lr{manifold}،
با تعداد داده‌های کم و هم‌چنین با تعداد ویژگی‌های کم به خوبی جواب می‌دهند، ولی با افزایش تعداد ویژگی‌ها یا افزایش تعداد داده‌ها به شدّت ناکارآمد می‌شوند.

درکنار روش‌های مبتنی بر
\lr{manifold}،
\cite{niu2014iterative}
روش
\lr{Dimension Growth (DG)}
را برای اجرای الگوریتم کاهش گرادیان با یک روش
\lr{greedy}
معرّفی می‌کند.

راه‌حل‌های قبلی همگی کند هستند و پیاده‌سازی آن‌ها دشوار است. بهترین راه‌حل برای حلّ این مسئله، الگوریتم‌
\lr{Interactive Spectral Method (ISM)}
است. در 
\cite{wu2018iterative}
از این الگوریتم برای حلّ مسئله‌ی خوشه‌بندی جایگزین استفاده شده است و مجموعه‌داده‌ای که خوشه‌بندی آن‌ها با روش
\lr{DG}،
حدود ۲ روز زمان می‌برد، در ۲ ثانیه خوشه‌بندی شده است.
این الگوریتم، به جای جستجو برای یافتن مقدار ویژه‌ها و بردار ویژه‌های ماتریس
\lr{kernel}،
به دنبال مقدار ویژه‌ها و بردار ویژه‌های یک ماتریس جایگزین و کوچک‌تر
\lr{$\Phi$}
می‌گردد. در نتیجه 
\lr{ISM}
می‌تواند این مسئله را سریع‌تر حل کند و پیاده‌سازی آسان‌تری هم دارد. ولی از لحاظ تئوری، فقط درمورد کرنل‌های گاوسی همگرایی و اعتبار این الگوریتم تضمین شده است
\cite{wu2018iterative}.

در این مقاله، تضمین‌های موجود درباره‌ی اجرای الگوریتم 
\lr{ISM}
روی کرنل‌های گاوسی، به خانواده‌ی بزرگ‌تری از کرنل‌ها (که 
\lr{ISM Family}
نامیده شده‌اند)
تعمیم داده شده است، ماتریس
\lr{$\Phi$}
برای هر عضو این خانواده محاسبه شده است و همچنین این تضمین‌ها درباره‌ی ترکیب‌های خطّی با ضرایب مثبت اعضای این خانواده اثبات شده‌اند. 

هم‌چنین چون تضمین‌های تئوری
\lr{ISM}
برای همه‌ی کرنل‌های خانواده‌ی
\lr{ISM}
اثبات شده است، می‌توان از الگوریتم
\lr{ISM}
برای حلّ مسائل گوناگون یادگیری استفاده کرد، مانند کاهش بعد نظارت‌شده
\cite{fukumizu2009kernel,barshan2011supervised,masaeli2010transformation}،
کاهش بعد بدون بی‌نظارت
\cite{scholkopf1998nonlinear,niu2011dimensionality}،
کاهش بعد با نظارت ناقص،
\cite{gangeh2016semi,chang2017clustering}
و مسائل دسته‌بندی جایگزین
\cite{wu2018iterative,niu2010multiple,niu2014iterative}.

