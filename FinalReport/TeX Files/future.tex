
\chapter{پیشنهاد‌ها}

برای ادامه‌ی کار این مقاله، می‌توان چند پیشنهاد به صورت زیر داد:
\begin{enumerate}
	\item 
	مقاله‌ی حاضر، در محاسبه‌ی
	\lr{$W^*$}،
	به ارائه‌ی یک شرط کافی (معادله‌ی 
	\ref{final})
	برای احراز شرایط مرتبه‌ی دوم مسأله‌ی بهینه‌سازی بسنده کرده و شرایط مطرح‌ شده، لازم نیستند. بهتر بود اگر شرایط لازم برای برقراری شرایط مرتبه‌ی دوم مسأله‌ی بهینه‌سازی بررسی و مطرح می‌شدند. ممکن است این شرایط به الگوریتم دیگری برای محاسبه‌ی نقطه‌ی بهینه منتهی شوند.
	\item 
	در
	\cite{zhang2012kernel}،
	روشی برای محاسبه‌ی میزان وابستگی دو متغیّر تصادفی به شرط یک متغیّر تصادفی دیگر، بر مبنای متر 
	\lr{HSIC}
	بیان شده است. می‌توانیم از این روش استفاده کرده و مسأله‌ای مشابه مسأله‌ی این مقاله، در حوزه‌ی 
	\lr{transfer learning}
	مطرح کرد. مسئله به این صورت است که فرض کنیم داده‌های
	\lr{$X_1,Y_1$}
	 با توزیع
	\lr{$p_1(x,y)$}
	داریم. حال می‌خواهیم داده‌های
	\lr{$X_2,Y_2$}
	 با توزیع
	\lr{$p_2(x,y)$}
	را کاهش بعد دهیم و در این فرآیند از اطّلاعات موجود در داده‌های قبلی هم استفاده کنیم. مسأله می‌تواند به صورت زیر فرمول‌بندی شود:
	\[\max_{W} \;\HSIC(X_2W,Y_2|X_1,Y_1)\qquad \text{\lr{s.t. }} W^\top W = I\]
	به عنوان مثال، فرض کنید 
	\lr{$X_1$}
	داده‌های مربوط به خریدهای افراد مختلف از یک فروشگاه اینترنتی ایرانی باشد و 
	\lr{$Y_1$}
	سنّ این افراد باشد. همچنین فرض کنید که 
	\lr{$X_2$}
	اطّلاعات خریدهای افراد دیگری در یک فروشگاه اینترنتی غیرایرانی باشد و 
	\lr{$Y_2$}
	سنّ این افراد باشد. هدف آنست که با توجّه به داده‌های فروشگاه ایرانی و همچنین برچسب‌های داده‌ها، ماتریس
	\lr{$W$}ای
	بیابیم که داده‌ها را به نحو مناسبی کاهش بعد دهد.
	
\item 
استفاده از ایده‌های روش‌های عددی بهینه‌سازی غیرمحدب برای حل 
	\lr{IKDR}
   
   در بهینه‌‌سازی غیرمحدب، روش‌های زیادی برای فرار از بهینه‌های محلی معرفی شده‌اند. یکی از این روش‌ها، روش 
	\lr{Gradual Non-Convexity}
   است. در این روش، ابتدا تخمینی محدب از تابع هدف بهینه‌سازی زده می‌شود، سپس مسئله‌ی بهینه‌سازی برای این تابع هدف حل شده و جواب آن به دست می‌آید. سپس تخمینی دقیق‌تر (و غیرمحدب‌تر) ارائه می‌شود و الگوریتم بهینه‌سازی از نقطه‌ی نهایی الگوریتم قبل، شروع به حرکت در جهت کمینه می‌کند. این عملیات تکرار می‌شود و در هر مرحله مقداری عدم تحدب به مسئله افزوده شده و از نقطه‌ی پایان مرحله قبل برای شروع بهینه‌سازی استفاده می‌شود. روش‌هایی مبتنی بر 
	\lr{Gradual Non-Convexity}
      تا کنون برای حل دسته‌های وسیعی از مسائل کاربرد داشته‌اند، به طور مثال می‌توان به مسئله‌ی بازسازی تنک و کمینه کردن نرم صفر با قید‌ خطی اشاره کرد 
   \cite{SL0}.
   بررسی عملکرد الگوریتم‌‌های مبتنی بر 
	\lr{Gradual Non-Convexity}
      و سایر ایده‌های موجود در بهینه‌سازی غیرمحدب برای حل این مسئله نیز یکی از راه‌های پیش‌رو برای توسعه و بهبود روش‌های حل مسئله‌ی 
	\lr{IKDR}
      است.

\end{enumerate}