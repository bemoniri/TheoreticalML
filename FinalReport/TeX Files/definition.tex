
\chapter{فرمول‌بندی مسئله}
در ابتدا، به تعریف  معیار استقلال هیلبرت- اشمیت
\lr{($\HSIC$)}
می‌پردازیم.
\section{معیار استقلال هیلبرت- اشمیت
	\lr{($\HSIC$)}
}

\lr{Hilbert-Schmidt Independence Criterion}
یا 
\lr{$\HSIC$}
معیاری برای سنجیدن استقلال دو متغیر تصادفی است. این معیار مشابه اطّلاعات متقابل، فاصله‌ای بین توزیع‌های
$P_{XY}$
و 
$P_XP_Y$
است. اطّلاعات متقابل از فاصله‌ی 
\lr{KL Divergence}
استفاده می‌کند، در مقابل،
\lr{HSIC}
از فاصله 
\lr{Maximum Mean Discrepancy (MMD)}
بهره می‌برد. ایده‌ی اصلی این فاصله آن است که در ابتدا، نگاشتی بر مبنای یک کرنل از فضای توزیع‌های احتمال به یک 
\lr{RKHS} 
و به شکل زیر ساخته می‌شود:
\begin{equation}
\begin{cases}
\mathcal{F}: \mathcal{P} \to \mathcal{H}_k\\
\mathcal{F}(P) = \mathbb{E}_{P}\left[ k(., X)\right]
\end{cases}
\end{equation}
در ادامه نیز از فاصله‌ی فضای هیلبرت ساخته شده توسط کرنل، به عنوان فاصله‌ی بین دو توزیع استفاده می‌کند.
\begin{equation}
\mathrm{MMD}(P, Q) = \norm{\mathbb{E}_{P}\left[ k(., X)\right] - \mathbb{E}_{Q}\left[ k(., X)\right]}_\mathcal{H}
\end{equation}
اگر نگاشت بین فضای توزیع‌ها و فضای هیلبرت، نگاشتی یک به یک باشد، در این نگاشت هیچ اطلاعاتی از توزیع از بین نمی‌رود و به راحتی می‌توان نشان داد که عبارت فوق، تمام خواص یک فاصله را دارا می‌باشد. به کرنل‌هایی که در آن‌ها نگاشت مذکور یک به یک است، کرنل مشخصه می‌گویند. کرنل گوسی مشهور‌ترین کرنل مشخصه است. برای یک کرنل مشخصه‌ی 
$k(., .)$،
تعریف می‌کنیم:
\begin{equation}
\mathrm{HSIC}_k(X, Y) \triangleq \mathrm{MMD}_k(P_XP_Y, P_{XY})
\end{equation}

برای دو متغیّر تصادفی 
$X$
و 
$Y$،
$\HSIC_k(X, Y)$
برابر با صفر می‌شود، اگر و تنها اگر $X$ و 
$Y$
مستقل باشند. بزرگ‌‌ بودن $\HSIC_k(X, Y)$
نیز به معنای وابستگی این دو متغیر است.
\subsection{تخمین 
	\lr{HSIC}
	از روی داده
}
حال فرض کنید مجموعه‌ی 
$\{(x_1, y_1), (x_2, y_2), \dots, (x_m, y_m)\}$
با توزیع مشترک
$P_{XY}$
داده شده است و قصد داریم بررسی کنیم که آیا $X$ و $Y$ مستقل هستند یا خیر. فرض کنید 
$X \in \mathbb{R}^{n\times d}$
و 
$Y \in \mathbb{R}^{n\times c}$
بردارهای مشاهدات باشند. ماتریس‌های گرام مربوط به $X$ و $Y$ را 
$K_X \in \mathbb{R}^{N\times N}$
و 
$K_Y \in \mathbb{R}^{N\times N}$
می‌نامیم. ماتریس $H$ را نیز ماتریس 
\lr{centering}
بگیرید،
$H = I_n - \frac{1}{n} \mathbf{1}_n\mathbf{1}_n^\top $.
در این صورت تخمین‌گیر زیر، یک تخمین‌گر نااریب از 
\lr{HSIC}
است:
\begin{equation}\label{hsic_estimate}
\mathrm{HSIC}(X, Y) = \frac{1}{(n-1)^2} \tr(K_X H K_Y H)
\end{equation}


فرض کنید 
$X \in \mathbb{R}^{n\times d}$
مجموعه‌ی داده‌ها با $d$ ویژگی و $n$ نمونه  و 
$Y \in \mathbb{R}^{n\times k}$
برچسب‌های این نمونه‌ها  ($k$ تعداد دسته‌ها است) باشند.  با استفاده از دو کرنل $k_x$ و $k_y$ داده‌شده، دو ماتریس گرام 
$K_X,\;K_Y \in \mathbb{R}^{n\times n}$
ساخته شده‌اند.
در مسئله‌ی 
\lr{IKDR}
قصد داریم ماتریس 
$W \in \mathbb{R}^{d\times q}$
را به نحوی بسازیم که 
$\mathrm{HSIC}(XW, Y)$
بیشینه شود (توجه کنید که 
$q<d$).
دلیل این امر آن است که می خوا‌هیم ماتریس کاهش‌‌بعدی را بیابیم که بعد از اعمال آن، ویژگی‌های کاهش‌یافته بیشترین اطلاعات ممکن در مورد برچسب‌ها را داشته باشند.
\begin{flalign*}
\HSIC(XW, Y) &= \frac{1}{(n-1)^2} \tr (K_{XW}HK_YH)\\
&= \frac{1}{(n-1)^2} \tr (HK_YHK_{XW})\\
&= \frac{1}{(n-1)^2} \tr (\Gamma K_{XW})
\end{flalign*}
که در آن 
$\Gamma = HK_YH$
است.

در نتیجه، مسئله‌ی 
\lr{IKDR}
را می‌توان به صورت زیر فرمول‌بندی کرد:
\begin{equation}\label{eq:obj_1}
\max_W \;\;  \tr(\Gamma K_{XW})\;\;\;\mathrm{s.t.}\;\;\;W^\top W=I
\end{equation}
شرط 
$W^\top W=I$
برای اجتناب از جواب‌های نامحدود و حذف ابهام مقیاس افزوده شده است.  این مسئله در حالت کلی، مسئله‌ای شدیداً غیر محدب است. هدف اصلی این مقاله و مقالات مشابه، ارائه‌ی الگوریتمی کارا و با تضمین همگرایی، برای حل این مسئله است.

\section{فرمول‌بندی مسائل مختلف یادگیری}
\subsection{کاهش بعد نظارت‌شده}
در مسئله‌ی کاهش بُعد نظارت‌شده
\cite{barshan2011supervised,masaeli2010transformation}،
ماتریس داده‌ها 
\lr{$(X)$}
و ماتریس برچسب‌ها
\lr{$(Y)$}
هردو معلوم هستند. در نتیجه کافیست که یک ماتریس
\lr{$W$}
بیابیم که 
\lr{$XW$}
و
\lr{$Y$}
تا حدّ امکان به هم وابسته شوند. در نتیجه باید
\lr{$\HSIC(XW,Y)$}
تا حدّ امکان بزرگ شود. از رابطه‌ی
(\ref{hsic_estimate})
می‌دانیم که بیشینه ‌شدن
\lr{$\HSIC(XW,Y)$}
معادل است با بیشینه‌ شدن
\lr{$\tr(K_X H K_Y H)$}.
و از آن‌جا که برچسب‌ها را داریم، ماتریس
\lr{$\Gamma = HK_YH$}
تماماً معلوم است. پس این مسئله به صورت زیر فرمول‌بندی می‌شود:
\begin{equation}
\max_W \;\;  \tr(\Gamma K_{XW})\;\;\;\mathrm{s.t.}\;\;\;W^\top W=I
\end{equation}
\subsection{کاهش بعد بدون نظارت}
در مسئله‌ی کاهش بعد بدون نظارت، برچسب‌های داده‌ها معلوم نیستند. در نتیجه باید هم
\lr{$W$}
و هم 
\lr{$Y$}
یاد گرفته شوند. اگر فرض کنیم
\lr{$K_Y = YY^\top$}،
در این صورت می‌توانیم با یک الگوریتم بازگشتی، در هر مرحله
\lr{$Y$}
را تخمین بزنیم و سپس بر مبنای
\lr{$Y$}
تخمین زده شده،
\lr{$W$} 
را به گونه‌ای بیابیم که 
\lr{$\HSIC(XW,Y)$}
بیشینه شود. امّا سؤال آنست که با فرض داشتن
\lr{$W$}،
چگونه
\lr{$Y$}
را به صورت مناسبی تخمین بزنیم؟ پاسخ این سؤال در
\cite{niu2011dimensionality}
بیان شده است. در این مرجع، روشی برای خوشه‌بندی بر مبنای فرمول‌بندی
\lr{$\HSIC$}
مطرح شده‌است. هنگامی که 
\lr{$Y$}
مشخّص شود، یافتن
\lr{$W$}
مانند بخش قبل است.

\subsection{کاهش بعد با نظارت ناقص}
در مسائل کاهش بعد با نظارت ناقص
\cite{chang2017clustering}،
برخی از برچسب‌ها برای همه‌ی داده‌ها داده شده‌اند و برخی دیگر نه، یعنی ماتریس برچسب‌ها به صورت
\lr{$\hat{Y} \in \mathbb{R}^{n \times r}$}
است. هم‌چنین فرض می‌شود که دو نمونه‌ی مشابه، برچسب مشابه دارند. در این مسائل، هدف آنست که داده‌ها را با کمک‌گرفتن از برچسب‌های داده‌شده خوشه‌‌بندی کنیم. خوشه‌بندی را می‌توان با استفاده از خوشه‌بندی طیفی 
\cite{von2007tutorial}
انجام داد و معیار 
\lr{$\HSIC$}
می‌تواند اطّلاعات برچسب‌هایی که داریم را هم وارد خوشه‌بندی کند. در نتیجه برای آن‌که هم‌زمان کیفیت خوشه‌بندی طیفی خوب باشد و 
\lr{$\HSIC(XW,\hat{Y} )$}
هم زیاد باشد، مسئله به این صورت فرمول‌بندی می‌شود:
\begin{flalign}
&\underset{W,Y}{\max}\;\; \tr (Y^T \mathcal{L}_W Y) + \mu \tr (K_{XW} H K_{\hat{Y}} H),
\label{eq:ssdr_1}
\\
&\text{\lr{s.t. }}  \quad \mathcal{L}_W = D^{-\frac{1}{2}}K_{XW} D^{-\frac{1}{2}},\; W^\top W = I,\; Y^\top Y = I
\end{flalign}
که
\lr{$\mu$}
ثابتی است که سهم جمله‌ی اوّل و دوم در بهینه‌سازی را مشخّص می‌کند و 
\lr{$D\in\mathbb{R}^{n\times n}$}
ماتریس درجه‌ی
\lr{$K_{XW}$}
است. یعنی یک ماتریس قطری که برای درایه‌های روی قطر آن داریم:
\lr{$D_{diag}= K_{XW}\textbf{1}_n$}.

مانند مسئله‌ی کاهش بعد بدون نظارت، بهینه‌سازی به صورت بازگشتی و روی
\lr{$W,Y$}
انجام می‌شود. از آن‌جا که جمله‌ی دوم به
\lr{$Y$}
وابسته نیست، وقتی که
\lr{$W$}
معلوم باشد، مسئله به یک خوشه‌بندی طیفی تقلیل می‌یابد و می‌توان
\lr{$Y$}
را محاسبه کرد. 

وقتی در هر دور از الگوریتم،
\lr{$Y$}
محاسبه شود، تعریف می‌کنیم:
\[\Psi = HK_{\hat{Y}}H,\qquad \Omega=D^{-\frac{1}{2}}Y Y^\top D^{-\frac{1}{2}}\]
و مسئله‌ی بهینه‌سازی
\ref{eq:ssdr_1}
به مسئله‌ی زیر تبدیل می‌شود:
\[\max_{W,Y} \tr [(\Omega + \mu \Psi) K_{XW} ] \qquad \text{\lr{s.t. }}W^\top W=I\]
و اگر تعریف کنیم
\lr{$\Gamma = \Omega + \mu \Psi$}
این بخش از مسئله به مسئله‌ی
\ref{eq:obj_1}
تبدیل می‌شود.
\subsection{دسته‌بندی جایگزین}
در مسائل دسته‌بندی جایگزین
\cite{niu2014iterative}،
یک مجموعه برچسب کامل
\lr{$\hat{Y} \in \mathbb{R}^{n \times k}$}
وجود دارد. این برچسب‌ها را «برچسب‌های اصلی» می‌نامیم.

در این دسته از مسائل، هدف آنست که یک مجموعه برچسب جایگزین بیابیم که کیفیت دسته‌بندی با این برچسب‌ها بالا باشد، ولی برچسب‌های جایگزین تا حدّ امکان با برچسب‌های اصلی متفاوت باشند. به بیان دیگر، هدف آنست که داده‌ها را از منظری دیگر دسته‌بندی کنیم. این مسئله مانند مسئله‌ی کاهش بعد با نظارت ناقص است، با این تفاوت که در آن مسئله هدف این بود که برچسب‌گذاری نهایی تا حدّ امکان با برچسب‌های موجود هم‌خوانی داشته باشد، ولی در این مسئله هدف اینست که برچسب‌گذاری نهایی با برچسب‌های موجود بیشترین فاصله را داشته باشد. در نتیجه فرمول‌بندی این مسئله هم مانند مسئله‌ی کاهش بعد با نظارت ناقص خواهد بود:
\begin{flalign}
&\underset{W,Y}{\max}\;\;  \tr (Y^\top \mathcal{L}_W Y) - 
\mu \tr (K_{XW} H K_{\hat{Y}} H),
\label{eq:ac_1}
\\
&\text{\lr{s.t. }}  \quad \mathcal{L}_W = D^{-\frac{1}{2}}K_{XW} D^{-\frac{1}{2}},\; W^\top W = I,\; Y^\top Y = I.
\end{flalign}
مشاهده می‌شود که تنها تغییر، علامت قبل از
\lr{$\mu$}
است. روش حلّ این مسئله کاملاً مشابه مسئله‌ی دسته‌بندی با نظارت ناقص است.
